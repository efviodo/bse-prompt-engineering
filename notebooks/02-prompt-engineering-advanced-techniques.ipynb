{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c7be754",
   "metadata": {},
   "source": [
    "<img src=\"../figures/header.png\" alt=\"Header\" width=\"450\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef16230",
   "metadata": {},
   "source": [
    "En los √∫ltimos meses, los LLMs (Large Language Models) han demostrado ser una herramienta sumamente √∫til y poderosa para un sin fin de tareas. De hecho, medio a√±o despu√©s del lanzamiento oficial de [ChatGPT](https://openai.com/blog/chatgpt), todav√≠a estamos rascando la superficie y descubriendo nuevos usos potenciales cada semana. Pero no todo es color de rosas, estos modelos todav√≠a no son lo suficientemente desarrollados para \"leer nuestras mentes\". Por esto, necesitan recibir instrucciones claras y libres de ambig√ºedades, para ejecutar tareas de forma exitosa. En pocas palabras, si queremos liberar el verdadero potencial de los LLMs necesitamos aprender a hablar su idioma.\n",
    "\n",
    "\n",
    "**¬øQu√© vamos a ver en este Notebook?**\n",
    "\n",
    "En este Notebook, vamos a experimentar dos t√©cnicas muy poderosas que cuando se utilizan en conjunto, logran obtener resultados realmente precisos: Chain-of-Thought (CoT) y Self-Consistency.\n",
    "\n",
    "**¬øQu√© herramientas vamos a utilizar?**\n",
    "\n",
    "Por simplicidad y afinidad a la implementaci√≥n actual de los productos de Cognitive Solutions, para los ejemplos de prompts vamos a utilizar el modelo m√°s reciente de OpenAI disponible a trav√©s de la API oficial (al momento de elaborar estos tutoriales dicho modelo era `gpt-3.5-turbo`). Adem√°s, para consumir la API de OpenAI vamos a utilizar el [sdk](https://pypi.org/project/openai/) oficial, que se encuentra disponible en pypi. Para el resto, funciones y l√≥gica adicional para el procesamiento de las respuestas del modelo, vamos a utilizar puro Python y alguna librer√≠a sencilla para alg√∫n problema espec√≠fico pero que no merece la pena mencionar todav√≠a.\n",
    "\n",
    "Por √∫ltimo, es necesario contar con una API Key v√°lida con cr√©ditos disponibles para realizar llamadas a la API de Open AI. Para m√°s informaci√≥n sobre c√≥mo generar y administrar las API Key puedes revisar la secci√≥n [OpenAI API Key setup](#openai-api-setup)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94d86f1",
   "metadata": {},
   "source": [
    "## √çndice\n",
    "<a class=\"anchor\" id=\"index\"></a>\n",
    "\n",
    "1. [Setups](#setups)\n",
    "    1. [OpenAI API Key setup](#openai-api-setup)\n",
    "    2. [Import & Utils](#imports)\n",
    "2. [Chain-of-Thought Prompting](#cot)\n",
    "    1. [Zero-Shot Chain of Thought](#zero-shot-cot)\n",
    "    2. [Few-Shot Chain of Thought](#few-shot-cot)\n",
    "3. [Self-Consistency](#self-consistency)\n",
    "4. [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accc501c",
   "metadata": {},
   "source": [
    "## Setups\n",
    "<a class=\"anchor\" id=\"setups\"></a>\n",
    "\n",
    "### 1. OpenAI API Key setup\n",
    "<a class=\"anchor\" id=\"openai-api-setup\"></a>\n",
    "\n",
    "ToDo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a813752",
   "metadata": {},
   "source": [
    "Para simplificar el manejo de las API Keys y evitar que queden \"harcodeadas\" en este notebook, utilizamos la librer√≠a [python-dotenv](https://pypi.org/project/python-dotenv/) que permite facilmente cargar variables de entorno definidas en un archivo local. En pocas palabras, lo √∫nico que necesitamos asegurarnos es que exista un archivo de nombre `.env` en la rai≈∫ de este proyecto, con el siguiente contenido:\n",
    "\n",
    "\n",
    "```yaml\n",
    "OPENAI_API_KEY=[A_VALID_OPENAI_API_KEY]\n",
    "```\n",
    "\n",
    "\n",
    "Como parte de la secci√≥n **Import&Utils** vamos a cargar esta variable de entorno utilizando los m√©todos `load_dotenv` y `find_dotenv` de esta libreria y finalmente setear las clases correspondientes del sdk de OpenAI para que utlicen la API Key cargada en memoria. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0111eaa",
   "metadata": {},
   "source": [
    "### 2. Imports & Utils\n",
    "<a class=\"anchor\" id=\"impots\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0955e5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')  # Be sure you have a .env file defined with your OPENAI_API_KEY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2fafb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\", temperature=1.0, top_p=1.0):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, # this is the degree of randomness of the model's output\n",
    "        top_p=top_p\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "def save(path: str, text: str):\n",
    "    \"\"\"\n",
    "    Helper function to save prompts and model responses as TXT files\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(f\"{path}.txt\", \"w\") as text_file:\n",
    "        text_file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2b538d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       ".label {\n",
       "  color: white;\n",
       "  padding: 5px;\n",
       "}\n",
       "\n",
       ".success {background-color: #04AA6D;} /* Green */\n",
       ".info {background-color: #2196F3;} /* Blue */\n",
       "\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Styles:\n",
    "\n",
    "This cell is to provide CSS styles for the model badge that indicates which model was used for some \n",
    "specific prompt.\n",
    "\"\"\"\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "\n",
    ".label {\n",
    "  color: white;\n",
    "  padding: 5px;\n",
    "}\n",
    "\n",
    ".success {background-color: #04AA6D;} /* Green */\n",
    ".info {background-color: #2196F3;} /* Blue */\n",
    "\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a138c78",
   "metadata": {},
   "source": [
    "## 1. Chain-of-Thought Prompting\n",
    "<a class=\"anchor\" id=\"cot\"></a>\n",
    "\n",
    "üîù [Ir al √çndice](#index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae23bba",
   "metadata": {},
   "source": [
    "Los LLM de gran dimensi√≥n como GPT-3 o PaLM han exhibido una capacidad impresionante para la comprensi√≥n del lenguaje natural y han demostrado ser extraordinariamente efectivos en tareas como extraer informaci√≥n de textos y generar respuestas en un estilo humano consistente. Aun as√≠, los LLM han demostrado ser bastante robustos al realizar tareas desconocidas cuando se incluyen algunos ejemplos como parte del prompt. Esta t√©cnica, popularizada como few-shot prompting por [Brown et al. (2020)](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf), ha probado ser muy buena en aumentar el rendimiento de modelos en varios benchmarks de referencia. Sin mencionar que ahorra dinero y tiempo al ajustar el modelo a un nuevo dominio espec√≠fico.\n",
    "\n",
    "Sin embargo, los estudios demostraron que modelos como estos disminuyen abruptamente su rendimiento en tareas comunes de razonamiento o problemas de matem√°ticas. A pesar de la capacidad de citar el poema √©pico griego La Odisea, estos modelos se trancan con problemas escolares fundamentales en l√≥gica y matem√°ticas.\n",
    "\n",
    "<img src=\"../figures/4.webp\" alt=\"Principles of Prompt Engineering\" />\n",
    "\n",
    "Entonces, ¬øqu√© opciones tenemos? ¬øEnviamos GPT-3 por un par de semestres a la escuela? Afortunadamente, existe una alternativa m√°s barata y menos embarazosa. \n",
    "\n",
    "De manera similar a c√≥mo los humanos abordan problemas complejos dividi√©ndolos en subproblemas m√°s simples y siguiendo una l√≠nea de razonamiento l√≥gico, podemos instruir a un modelo de lenguaje para que haga lo mismo. Este enfoque fue explorado por Wei J et al. en [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (2022)](https://arxiv.org/abs/2201.11903). Demostr√≥ resultados impresionantes en varios benchmark de referencia, confirmando Chain-of-Thought (CoT) como un enfoque s√≥lido para mejorar el rendimiento de LLM en tareas de razonamiento comunes.\n",
    "\n",
    "En la imagen a continuaci√≥n (tomada del art√≠culo de Wei J), un modelo LLM se apresura a llegar a una conclusi√≥n incorrecta al calcular la cantidad de manzanas que quedan en una cafeter√≠a. Esto sucede incluso cuando se proporciona un problema de razonamiento similar usando pelotas de tenis como parte del contexto.\n",
    "\n",
    "Sin embargo, cuando se incluye en el contexto (CoT) un proceso paso a paso para resolver el problema, el modelo puede llegar con precisi√≥n a una soluci√≥n v√°lida.\n",
    "\n",
    "<img src=\"../figures/3.png\" alt=\"Principles of Prompt Engineering\" />\n",
    "\n",
    "Utilizando CoT, modelos de lenguaje de escala suficiente (equivalente en par√°metros a 100B) pueden:\n",
    "\n",
    "1. Descomponer problemas de varios pasos en pasos intermedios, lo que significa que se pueden asignar c√°lculos adicionales a problemas que requieren m√°s pasos de razonamiento.\n",
    "\n",
    "2. Proporcionar una ventana interpretable del comportamiento del modelo, sugiriendo c√≥mo podr√≠a haber llegado a una respuesta particular y brindando oportunidades para depurar d√≥nde sali√≥ mal el camino del razonamiento.\n",
    "\n",
    "3. Extender el campo de aplicaci√≥n a problemas matem√°ticos, razonamiento de sentido com√∫n y manipulaci√≥n simb√≥lica.\n",
    "\n",
    "Este m√©todo se puede utilizar proporcionando ejemplos de razonamiento al modelo (few-shot) o ning√∫n ejemplo (zero-shot). Veamos ambos sabores en la pr√°ctica con un ejemplo de aplicaci√≥n real.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8d28ef",
   "metadata": {},
   "source": [
    "### 1.1. Zero-Shot Chain of Thought\n",
    "<a class=\"anchor\" id=\"zero-shot-cot\"></a>\n",
    "\n",
    "üîù [Ir al √çndice](#index)\n",
    "\n",
    "Imagina que estamos desarrollando una nueva aplicaci√≥n de compras para Walmart, con la caracter√≠stica innovadora de comparar y seleccionar qu√© producto debe comprar en funci√≥n de los precios y atributos de las alternativas de diferentes marcas.\n",
    "\n",
    "Para ilustrar el problema, centr√©monos en una lista reducida de todas las opciones en [Barrasd de Jabon](https://www.walmart.com/browse/personal-care/bar-soap/1005862_1071969_1694681_4735846?povid=GlobalNav_rWeb_PersonalCare_BathBody_BarSoap) que tiene Walmart en sus tiendas. Como ves, tenemos packs desde 1 barra hasta 14 barras y gran variedad de marcas y precios (desde opciones baratas hasta opciones caras)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9c0a5b",
   "metadata": {},
   "source": [
    "**Products üõí**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "668b55c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = f\"\"\"\n",
    "---\n",
    "Dove Men+Care 8 bars pack\n",
    "$ 9.99\n",
    "\n",
    "---\n",
    "Dove Beauty Bar 4 bars pack\n",
    "$ 6.47\n",
    "\n",
    "---\n",
    "Dove Beauty Bar 1 bars\n",
    "$ 1.47\n",
    "\n",
    "---\n",
    "Dove Beauty Bar 14 bars pains\n",
    "$ 16\n",
    "\n",
    "---\n",
    "Yardley London Soap Bar (Pack of 10)\n",
    "$ 19.99\n",
    "\n",
    "---\n",
    "Dr. Squatch All Natural Bar Soap for Men, 5 Bar Variety Pack\n",
    "$46.45\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94ba528",
   "metadata": {},
   "source": [
    "Para determinar qu√© opci√≥n es m√°s conveniente, podemos calcular el precio por barra en cada alternativa (precio unitario). A continuaci√≥n, seleccionar la opci√≥n m√°s econ√≥mica. Siguiendo este razonamiento, descubrimos que Dove Beauty Bar 14 barras es la opci√≥n m√°s econ√≥mica con un precio unitario de $1,14 (-22% por barra que Dove Beauty Bar 1 barras)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252f1cf1",
   "metadata": {},
   "source": [
    "**Reasoning üß†**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214b7040",
   "metadata": {},
   "source": [
    "```\n",
    "- Dove Men+Care 8 bars pack: $1,24875 / unit\n",
    "- Dove Beauty Bar 4 bars pack: 1,6175 / unit\n",
    "- Dove Beauty Bar 1 bars: 1,47 / unit\n",
    "- Dove Beauty Bar 14 bars pains: $1,142857 per / unit\n",
    "- Yardley London Soap Bar (Pack of 10): 1,999 / unit\n",
    "- Dr. Squatch All Natural Bar Soap for Men, 5 Bar Variety Pack: $9,29 / unit\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea1839c",
   "metadata": {},
   "source": [
    "Veamos si GPT-4 es lo suficientemente inteligente como para resolver este problema con un simple prompt. Adem√°s, con el esp√≠ritu de hacer ejemplos m√°s limpios y f√°ciles de leer, separaremos las instrucciones del sistema del resto de la entrada usando Python [string literals](https://docs.python.org/3/reference/lexical_analysis.html#formatted-string-literals)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6c0478",
   "metadata": {},
   "source": [
    "**Prompt üí¨**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32df22ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You will be provided with a list of various product presentations delimited by <>. Each product presentation is accompanied by its price. Your task is to identify which is the best option based on the price.\n",
    "\n",
    "<{products}>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32df29b7",
   "metadata": {},
   "source": [
    "**Response ü§ñ** &nbsp;&nbsp;<span class=\"label success\">GPT-4</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f4e3b8",
   "metadata": {},
   "source": [
    "```python\n",
    "The best option based on price is Dove Beauty Bar 1 bars for $1.47.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40a18e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the prices provided, the best option would be the Dove Beauty Bar 4 bars pack priced at $6.47.\n"
     ]
    }
   ],
   "source": [
    "# Your turn\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fff126",
   "metadata": {},
   "source": [
    "El modelo eligi√≥ la opci√≥n m√°s barata en funci√≥n del precio total en lugar de calcular el precio unitario e identificar una oportunidad de ahorro comprando m√°s barras. Vale la pena se√±alar cierta ambig√ºedad en la pregunta. ¬øQu√© significa la mejor opci√≥n? ¬øSignifica la m√°s barata o la que maximiza un ahorro de dinero a largo plazo?\n",
    "\n",
    "Reescribamos el prompt indicando al modelo que siga una cadena de pensamiento para resolver este problema agregando las palabras m√°gicas _‚Äúreasoning step by step‚Äù_. Adem√°s subrayando expl√≠citamente el objetivo de encontrar una oportunidad para ahorrar dinero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6714c307",
   "metadata": {},
   "source": [
    "**Prompt üí¨**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b152551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You will be provided with a list of various product presentations delimited by <>. Each product presentation is accompanied by its price. Your task is to identify which presentation offers the best money-saving opportunity, reasoning step-by-step:\n",
    "\n",
    "<{products}>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aade2a",
   "metadata": {},
   "source": [
    "**Response ü§ñ** &nbsp;&nbsp;<span class=\"label success\">GPT-4</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb6837a",
   "metadata": {},
   "source": [
    "```python\n",
    "The first step is to compare price per unit for products of the same type (i.e. bars of soap). \n",
    "\n",
    "- Dove Men+Care 8 bars pack: $9.99 √∑ 8 bars = $1.25 per bar\n",
    "- Dove Beauty Bar 4 bars pack: $6.47 √∑ 4 bars = $1.62 per bar\n",
    "- Dove Beauty Bar 1 bars: $1.47 per bar\n",
    "- Dove Beauty Bar 14 bars pack: $16 √∑ 14 bars = $1.14 per bar\n",
    "- Yardley London Soap Bar (Pack of 10): $19.99 √∑ 10 bars = $1.99 per bar\n",
    "- Dr. Squatch All Natural Bar Soap for Men, 5 Bar Variety Pack: $46.45 √∑ 5 bars = $9.29 per bar\n",
    "\n",
    "Based on this analysis, the best money-saving opportunity is the Dove Beauty Bar 14 bars pack, which offers the lowest price per bar at $1.14.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9089232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To identify the best money-saving opportunity, we need to compare the prices of the different product presentations. \n",
      "\n",
      "The first option is the Dove Men+Care 8 bars pack, priced at $9.99.\n",
      "\n",
      "The second option is the Dove Beauty Bar 4 bars pack, priced at $6.47.\n",
      "\n",
      "The third option is the Dove Beauty Bar 1 bar, priced at $1.47.\n",
      "\n",
      "The fourth option is the Dove Beauty Bar 14 bars pack, priced at $16.\n",
      "\n",
      "The fifth option is the Yardley London Soap Bar (Pack of 10), priced at $19.99.\n",
      "\n",
      "The sixth option is the Dr. Squatch All Natural Bar Soap for Men, 5 Bar Variety Pack, priced at $46.45.\n",
      "\n",
      "By comparing the prices, we can see that the Dove Beauty Bar 1 bar is the cheapest option at $1.47. However, it is important to consider the number of bars included in each pack to make a fair comparison.\n",
      "\n",
      "The Dove Men+Care 8 bars pack offers 8 bars for $9.99, making each bar cost approximately $1.25 (9.99 / 8).\n",
      "\n",
      "The Dove Beauty Bar 4 bars pack offers 4 bars for $6.47, making each bar cost approximately $1.62 (6.47 / 4).\n",
      "\n",
      "The Dove Beauty Bar 1 bar costs $1.47.\n",
      "\n",
      "The Dove Beauty Bar 14 bars pack offers 14 bars for $16, making each bar cost approximately $1.14 (16 / 14).\n",
      "\n",
      "The Yardley London Soap Bar pack offers 10 bars for $19.99, making each bar cost approximately $1.99 (19.99 / 10).\n",
      "\n",
      "The Dr. Squatch All Natural Bar Soap for Men, 5 Bar Variety Pack, priced at $46.45, offers 5 bars. This makes each bar cost approximately $9.29 (46.45 / 5).\n",
      "\n",
      "Based on the calculations, the best money-saving opportunity is the Dove Beauty Bar 14 bars pack, which offers each bar at approximately $1.14.\n"
     ]
    }
   ],
   "source": [
    "# Now is your turn!\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1c07ab",
   "metadata": {},
   "source": [
    "Notar que con el nuevo prompt, el modelo da una respuesta correcta y, al mismo tiempo, tenemos un razonamiento paso a paso que es m√°s f√°cil de debugear y encontrar posibles errores. A√∫n m√°s, es bastante impresionante c√≥mo agregando las palabras m√°gicas \"reasoning step-by-step\" en el prompt, se produce una diferencia significativa en la salida del modelo. ¬øCu√°l es el truco?\n",
    "\n",
    "El truco es obligar al modelo a generar una cadena de razonamiento paso a paso (Chain of Thought) en lugar de simplemente generar un valor num√©rico o booleano (resultado final). Al adoptar esta estrategia, inicialmente guiamos el modelo hacia resultados de razonamiento l√≥gico, lo que permite que el modelo base sus respuestas en problemas similares encontrados durante el entrenamiento. En segundo lugar, ayudamos al modelo solicitando que divida los problemas m√°s grandes en otros m√°s peque√±os y manejables. Luego, se requiere que el modelo produzca resultados coherentes para subproblemas simples, como identificar precios y la cantidad de barras en cada paquete, calcular el precio unitario y, finalmente, realizar comparaciones. Adem√°s, debido a la forma en que los modelos de lenguaje autorregresivos como GPT-3 o GPT-4 generan secuencias token por token, donde cada nuevo token se genera en funci√≥n de todos los tokens generados anteriormente, la cadena de pensamiento en s√≠ tiene una fuerte influencia positiva. en la generaci√≥n de un resultado final coherente.\n",
    "\n",
    "En otras palabras, si, a trav√©s de un razonamiento v√°lido, llegamos a un punto a medio camino de la soluci√≥n correcta, nuestras posibilidades de llegar a esta soluci√≥n son mayores que cuando empezamos a razonar sobre el problema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa46fbdb",
   "metadata": {},
   "source": [
    "### 1.2. Few-Shot Chain of Thought\n",
    "<a class=\"anchor\" id=\"few-shot-cot\"></a>\n",
    "\n",
    "üîù [Ir al √çndice](#index)\n",
    "\n",
    "Mientras que en Zero-Shot Chain-of-Thought dejamos al modelo libre para elaborar una cadena de razonamiento coherente y v√°lida que conduzca a un resultado correcto, un segundo tipo de prompting llamado Few-Shot Chain-of-Thought proporciona algunos ejemplos de un problema. junto con una respuesta de razonamiento, como parte del prompt. De esta forma, se impone al modelo una forma de razonamiento, limitando las posibles respuestas.\n",
    "\n",
    "Volviendo a nuestra aplicaci√≥n de Walmart, supongamos que queremos identificar qu√© presentaci√≥n de refresco Coca Cola tiene la mejor relaci√≥n precio por onza l√≠quida. Para ello, incorporaremos al prompt una cadena de razonamiento similar, para encontrar las mejores presentaciones de las papas fritas Lay's."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191aef5d",
   "metadata": {},
   "source": [
    "**Prompt üí¨**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63071076",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Q: Walmart has the following presentations for Lay's Classic Potato Chips: a 13 oz bag for $4.98, an 8 oz bag for $2.76, and a pack of x10 units 1 oz each for $6.82. Which presentation has the lowest price by ounce?\n",
    "\n",
    "A: Presentation 1 costs 4.98 so has a price of 4.98/13 = 0,383076923 per ounce. Presentation 2 costs 2.76 so has a price of 2.76/8 = 0,345 / per ounce. Presentation 3 costs 6.82 and includes 10 packages so has a price of 6.82/(1 x 10) = 0.682 per ounce. In conclusion, the cheapest presentation is the second one with a price of 0.345 per ounce.\n",
    " \n",
    "Q: Walmart has the following presentation for Coca-Cola Soda:\n",
    "an x24 pack of 12 fl oz cans for $11.86, an x12 pack of 12 fl oz cans for $7.46, and an x6 pack of 16.9 fl oz bottles for $4.98. Which presentation has the lowest price by fluid ounce?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04834394",
   "metadata": {},
   "source": [
    "Se alienta al lector, a que hagas tus propios experimentos y encuentre la mejor oferta. Encontrar√° que el modelo est√° en lo correcto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f3e99c",
   "metadata": {},
   "source": [
    "**Response ü§ñ** &nbsp;&nbsp;<span class=\"label success\">GPT-4</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f372acc9",
   "metadata": {},
   "source": [
    "```python\n",
    "A: The 24-pack contains a total of 24 x 12 = 288 fluid ounces and costs $11.86, so the price per fluid ounce is 11.86/288 = $0.041 per fluid ounce. The 12-pack contains a total of 12 x 12 = 144 fluid ounces and costs $7.46, so the price per fluid ounce is 7.46/144 = $0.052 per fluid ounce. The 6-pack contains a total of 6 x 16.9 = 101.4 fluid ounces and costs $4.98, so the price per fluid ounce is 4.98/101.4 = $0.049 per fluid ounce. Therefore, the 24-pack has the lowest price per fluid ounce.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb82228f",
   "metadata": {},
   "source": [
    "Por m√°s incre√≠ble que parezca, este sencillo truco le permite al modelo inferir una cadena de razonamiento similar y lograr resultados a√∫n m√°s precisos que con la t√©cnica anterior. Adem√°s, tenga en cuenta que no estamos codificando un procedimiento paso a paso para resolver el problema. Por lo tanto, este enfoque deber√≠a ser, en teor√≠a, lo suficientemente flexible para resolver cualquier desaf√≠o similar, bas√°ndose √∫nicamente en las habilidades de razonamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ce15b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: To determine the lowest price by fluid ounce, we need to calculate the price per fluid ounce for each presentation.\n",
      "\n",
      "Presentation 1: 24-pack of 12 fl oz cans for $11.86\n",
      "Price per fluid ounce = 11.86 / (24 x 12) = 0.163 / fl oz\n",
      "\n",
      "Presentation 2: 12-pack of 12 fl oz cans for $7.46\n",
      "Price per fluid ounce = 7.46 / (12 x 12) = 0.052 / fl oz\n",
      "\n",
      "Presentation 3: 6-pack of 16.9 fl oz bottles for $4.98\n",
      "Price per fluid ounce = 4.98 / (6 x 16.9) = 0.049 / fl oz\n",
      "\n",
      "Based on the calculations, Presentation 3 has the lowest price per fluid ounce at $0.049.\n"
     ]
    }
   ],
   "source": [
    "# Now is your turn!\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b055cc",
   "metadata": {},
   "source": [
    "## 2. Self-Consistency\n",
    "<a class=\"anchor\" id=\"self-consistency\"></a>\n",
    "\n",
    "üîù [Ir al √çndice](#index)\n",
    "\n",
    "Hay algunos escenarios en los que un LLM puede no generar respuestas consistentes, incluso usando CoT y para el mismo prompt, es posible que el modelo se confunda entre ejecuciones y devuelva una respuesta inconsistente. En este contexto, [Self-Consistency](https://arxiv.org/abs/2203.11171) es un enfoque simple pero efectivo, que consiste simplemente en preguntar al modelo con el mismo prompt varias veces y tomar el resultado de la mayor√≠a, como respuesta final. La combinaci√≥n de esta t√©cnica con CoT ayuda a obtener resultados m√°s robustos y predecibles.\n",
    "\n",
    "Por ejemplo, consideremos el ejemplo de las manzanas en la cafeter√≠a presentado en el documento Chain-of-Thought. En lugar de simplemente llamar al modelo una vez con el indicador de entrada, podemos llamar al modelo tres veces y tomar la respuesta mayoritaria como la respuesta final (ver imagen a continuaci√≥n).\n",
    "\n",
    "<img src=\"../figures/5.png\" alt=\"Principles of Prompt Engineering\" width=\"700\"/>\n",
    "\n",
    "\n",
    "Al adoptar la respuesta de la mayor√≠a, la probabilidad de que el modelo devuelva una salida incorrecta durante una ejecuci√≥n espec√≠fica se reduce significativamente. El efecto de esta t√©cnica se potencia al aumentar la repetici√≥n. Sin embargo, la ejecuci√≥n repetida del prompt implica una mayor cantidad de llamadas al modelo, lo que lleva a un mayor consumo de hardware. En el caso de modelos de lenguajes grandes como GPT-3, esto tambi√©n se traduce en m√°s llamadas a la API y, en consecuencia, m√°s dinero gastado, todo para generar una √∫nica respuesta.\n",
    "\n",
    "Por este motivo, a la hora de dise√±ar sistemas que utilicen LLM, es fundamental equilibrar la robustez y el ahorro de llamadas al modelo, utilizando esta t√©cnica solo cuando sea estrictamente necesario. Por ejemplo, cuando se necesita un alto nivel de robustez en la respuesta del sistema.\n",
    "\n",
    "Para ilustrar el potencial de esta t√©cnica, volvamos a nuestra aplicaci√≥n de Walmart para analizar un caso particular: [huevos](https://www.walmart.com/browse/food/eggs/976759_9176907_1001469?povid=GlobalNav_rWeb_Grocery_DairyEggs_Eggs).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2c4486",
   "metadata": {},
   "source": [
    "**Products üõí**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb2794c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = f\"\"\"\n",
    "---\n",
    "Great Value Cage Free Large AA White Eggs, 6 Count\n",
    "\n",
    "$ 1.07\n",
    "\n",
    "---\n",
    "Happy Egg Organic Free Range Large Brown Eggs, 12 Count\n",
    "\n",
    "$ 5.16\n",
    "\n",
    "---\n",
    "Great Value Cage Free Large AA White Eggs, 12 Count\n",
    "\n",
    "$ 1.86\n",
    "\n",
    "---\n",
    "Eggland's Best Farm Fresh Cage Free Large White Grade AA Eggs, 18 Count\n",
    "\n",
    "$ 2.69\n",
    "\n",
    "---\n",
    "Marketside Large Cage Free Brown Eggs, 12 Count\n",
    "\n",
    "$ 2.67\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943044d4",
   "metadata": {},
   "source": [
    "Agreguemos algunas preferencias de usuario como parte del prompt, para hacer el problema un poquito m√°s desafiante para el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa91b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "preferences = f\"\"\"\n",
    "- Only consider grade AA eggs\n",
    "- Only consider eggs produced on cage-free farms\n",
    "- The client needs a minimum of 6 eggs and a maximum of 12\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3c9e12",
   "metadata": {},
   "source": [
    "Utilizando modelos poderosos como GPT-3/GPT-4, un prompt Zero-Shot CoT como el siguiente deber√≠a ser suficiente para obtener resultados descentes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446a7977",
   "metadata": {},
   "source": [
    "**Prompt üí¨**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fe85afe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preferences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mYou will be provided with a list of various product presentations delimited by <>. Each product presentation is accompanied by its price. Your task is to identify which presentation offers the best money-saving opportunity, reasoning step-by-step:\u001b[39m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;124mAlso, consider the following user preferences to select the most suitable ooption:\u001b[39m\n\u001b[1;32m      5\u001b[0m \n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mpreferences\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m<\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproducts\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preferences' is not defined"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "You will be provided with a list of various product presentations delimited by <>. Each product presentation is accompanied by its price. Your task is to identify which presentation offers the best money-saving opportunity, reasoning step-by-step:\n",
    "\n",
    "Also, consider the following user preferences to select the most suitable ooption:\n",
    "\n",
    "{preferences}\n",
    "\n",
    "<{products}>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c401796",
   "metadata": {},
   "source": [
    "**Response ü§ñ** &nbsp;&nbsp;<span class=\"label success\">GPT-4</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d642d3e4",
   "metadata": {},
   "source": [
    "```python\n",
    "(1)\n",
    "Considering the user preferences, we first filter the list of eggs for only grade AA and cage-free options, which gives us the following choices:\n",
    "\n",
    "* Great Value Cage Free Large AA White Eggs, 6 Count: $1.07\n",
    "* Great Value Cage Free Large AA White Eggs, 12 Count: $1.86\n",
    "* Eggland's Best Farm Fresh Cage Free Large White Grade AA Eggs, 18 Count: $2.69\n",
    "* The Happy Egg and Marketside options are discarded because their grade is not specified.\n",
    "\n",
    "Next, we consider the amount of eggs. The client needs a minimum of 6 eggs and a maximum of 12. Therefore, we can discard the Eggland's Best 18 count option, as it exceeds the maximum required amount.\n",
    "\n",
    "Now, we compare the remaining two options:\n",
    "\n",
    "* Great Value Cage Free Large AA White Eggs, 6 Count: $1.07\n",
    "* Great Value Cage Free Large AA White Eggs, 12 Count: $1.86\n",
    "* Although the 6-count pack fits the minimum requirement, the 12-count pack offers a better price per egg. For the 6-count pack, each egg costs $1.07 / 6 = ~$0.178. For the 12-count pack, each egg costs $1.86 / 12 = ~$0.155.\n",
    "\n",
    "Therefore, considering both the user's preferences and the cost per egg, the \"Great Value Cage Free Large AA White Eggs, 12 Count\" for $1.86 is the best money-saving opportunity.\n",
    "\n",
    "(2)\n",
    "Let's go through each product one by one, considering the user's preferences:\n",
    "\n",
    "1. Great Value Cage Free Large AA White Eggs, 6 Count - $1.07\n",
    "    These are grade AA eggs from a cage-free farm, so they meet the user's criteria. The price per egg is $1.07/6 = $0.178.\n",
    "\n",
    "2. Happy Egg Organic Free Range Large Brown Eggs, 12 Count - $5.16\n",
    "    These are not specified as grade AA, so we can't consider them based on user's preferences.\n",
    "\n",
    "3. Great Value Cage Free Large AA White Eggs, 12 Count - $1.86\n",
    "    These are grade AA eggs from a cage-free farm and the quantity is within the range the user wants. The price per egg is $1.86/12 = $0.155.\n",
    "\n",
    "4. Eggland's Best Farm Fresh Cage Free Large White Grade AA Eggs, 18 Count - $2.69\n",
    "    These are grade AA eggs from a cage-free farm. However, the quantity is more than the user wants (18), so we can't consider this option.\n",
    "\n",
    "5. Marketside Large Cage Free Brown Eggs, 12 Count - $2.67\n",
    "    These are not specified as grade AA, so we can't consider them based on user's preferences.\n",
    "\n",
    "Based on the preferences, we are only left with two choices: Great Value Cage Free Large AA White Eggs, either 6 Count or 12 Count. However, the 12 Count package offers the best money-saving opportunity, because it has a lower cost per egg ($0.155 per egg) than the 6 Count package ($0.178 per egg). Additionally, the 12 Count package still falls within the user's desired quantity range of 6 to 12 eggs. Therefore, the best option for the user is the \"Great Value Cage Free Large AA White Eggs, 12 Count\" at $1.86.\n",
    "\n",
    "(3)\n",
    "Given the user's preferences, we need to find the product which:\n",
    "\n",
    "1. Contains only grade AA eggs\n",
    "2. Is produced on cage-free farms\n",
    "3. Offers a minimum of 6 eggs and a maximum of 12\n",
    "\n",
    "Let's consider each product:\n",
    "\n",
    "1. \"Great Value Cage Free Large AA White Eggs, 6 Count\" - $ 1.07. This option matches all the user's preferences and costs $1.07 for 6 eggs, which is approximately $0.178 per egg.\n",
    "\n",
    "2. \"Happy Egg Organic Free Range Large Brown Eggs, 12 Count\" - $ 5.16. This product does not specify the grade of the eggs, so we will disregard this option.\n",
    "\n",
    "3. \"Great Value Cage Free Large AA White Eggs, 12 Count\" - $ 1.86. This option also matches the user's preferences and costs $1.86 for 12 eggs, which is approximately $0.155 per egg.\n",
    "\n",
    "4. \"Eggland's Best Farm Fresh Cage Free Large White Grade AA Eggs, 18 Count\" - $ 2.69. While this product matches the egg grade and production method preferences, it contains more than the maximum amount of eggs requested by the user (18 instead of 12). Therefore, we disregard this option.\n",
    "\n",
    "5. \"Marketside Large Cage Free Brown Eggs, 12 Count\" - $ 2.67. This product does not specify the grade of the eggs, so we will disregard this option.\n",
    "\n",
    "Therefore, the product that provides the best money-saving opportunity while meeting the user's preferences is \"Great Value Cage Free Large AA White Eggs, 12 Count\" for $1.86. This option offers more eggs for a lower cost per egg compared to the other suitable option.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f38e4f",
   "metadata": {},
   "source": [
    "A pesar de que el modelo no produce resultados incorrectos en ninguna de sus ejecuciones (con un razonamiento preciso en las tres instancias), este ejemplo enfatiza el potencial de variabilidad en las respuestas de un LLM. Adem√°s, subraya el valor del m√©todo self-consistency en la reducci√≥n de dicha variabilidad.\n",
    "\n",
    "M√°s en general, se ha demostrado que el m√©todo self-consistency mejora los resultados en tareas de razonamiento aritm√©tico, de sentido com√∫n y simb√≥lico. Incluso cuando se descubri√≥ que el CoT regular era ineficaz, la autoconsistencia a√∫n pod√≠a mejorar los resultados (consulte [resultados experimentales](https://arxiv.org/abs/2203.11171)).\n",
    "\n",
    "Para concluir esta t√©cnica avanzada, te animo a que la pruebes t√∫ mismo, por ejemplo, agregando m√°s productos a la lista o quitando el ‚Äúrazonamiento paso a paso‚Äù para darle al modelo m√°s espacio para cometer errores. Adem√°s, intente aplicar la autoconsistencia a los problemas de la vida real en los que est√° trabajando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e3c0718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: Presentation 1 costs $11.86 and includes 24 cans, so the price per fluid ounce is $11.86/(24 x 12) = $0.041 per fluid ounce. Presentation 2 costs $7.46 and includes 12 cans, so the price per fluid ounce is $7.46/(12 x 12) = $0.051 per fluid ounce. Presentation 3 costs $4.98 and includes 6 bottles, so the price per fluid ounce is $4.98/(6 x 16.9) = $0.049 per fluid ounce. In conclusion, the cheapest presentation is the first one with a price of $0.041 per fluid ounce.\n"
     ]
    }
   ],
   "source": [
    "# Now is your turn!\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b14417a",
   "metadata": {},
   "source": [
    "## References\n",
    "<a class=\"anchor\" id=\"references\"></a>\n",
    "\n",
    "* [Prompt Engineering 101: Prompts, Outputs and Notebooks with code]()\n",
    "\n",
    "* [Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., & Zhou, D. (2022). Chain of thought prompting elicits reasoning in large language models]()\n",
    "\n",
    "* [Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., & Zhou, D. (2022). Self-consistency improves chain of thought reasoning in language models]()\n",
    "\n",
    "* [Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. (2020). Language models are few-shot learners]()\n",
    "\n",
    "* [ChatGPT Prompt Engineering for Developers]() by DeepLearning.ai\n",
    "\n",
    "* [Building Systems with the ChatGPT API]() by DeepLearning.ai\n",
    "\n",
    "* [The Full Stack LLM Bootcamp]()\n",
    "\n",
    "* Prompt Engineering Guide\n",
    "https://www.promptingguide.ai/\n",
    "\n",
    "* Introductory Course on Prompting\n",
    "https://learnprompting.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt-engineering",
   "language": "python",
   "name": "prompt-engineering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
